{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 1: Load the Model\n",
    "num_output_classes = 24  # Same as used during training\n",
    "\n",
    "model = timm.create_model(\n",
    "    \"vit_base_patch8_224\",\n",
    "    pretrained=True,\n",
    "    in_chans=1,\n",
    "    num_classes=num_output_classes,\n",
    "    patch_size=224,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"models/best_model.pt\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/arnavs19/attention-rollout-for-vision-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(224, 224), stride=(224, 224))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (patch_drop): Identity()\n",
      "  (norm_pre): Identity()\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (fc_norm): Identity()\n",
      "  (head_drop): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=768, out_features=24, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the Image\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"L\")  # Convert to grayscale\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to the model's input size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]),  # Normalize for grayscale\n",
    "    ])\n",
    "    input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return input_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Hook for Attention Weights\n",
    "attention_maps = []\n",
    "\n",
    "def get_attention_maps(module, input, output):\n",
    "    attention_maps.append(output)\n",
    "\n",
    "# Register hook to all attention layers\n",
    "for name, module in model.named_modules():\n",
    "    if \"attn_drop\" in name:  # Look for attention layers\n",
    "        module.register_forward_hook(get_attention_maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avs20\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\timm\\models\\vision_transformer.py:92: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  x = F.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Forward Pass to Extract Attention Maps\n",
    "image_path = r\"C:\\Users\\avs20\\Documents\\GitHub\\facemap\\cam1_G7c1_1_img2041_rescale_augmented.jpg\"  # Replace with your image path\n",
    "input_tensor = preprocess_image(image_path)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = model(input_tensor)  # Run the model to get attention weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): Module(\n",
       "    (proj): Conv2d(1, 768, kernel_size=(224, 224), stride=(224, 224))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Module(\n",
       "    (0): Module(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Module(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Module(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Module(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Module(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Module(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Module(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Module(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Module(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Module(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Module(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Module(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Module(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Module(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Module(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Module(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Module(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Module(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Module(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Module(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Module(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Module(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Module(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Module(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Module(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Module(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Module(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Module(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Module(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Module(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Module(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Module(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Module(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Module(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Module(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "\n",
    "# Step 1: Load the Model\n",
    "num_output_classes = 24  # Same as used during training\n",
    "\n",
    "\n",
    "timm.layers.set_fused_attn(False) # disable F.sdpa so softmax node is exposed\n",
    "\n",
    "mm = timm.create_model(\n",
    "    \"vit_base_patch8_224\",\n",
    "    pretrained=True,\n",
    "    in_chans=1,\n",
    "    num_classes=num_output_classes,\n",
    "    patch_size=224,\n",
    ")\n",
    "mm.load_state_dict(torch.load(\"models/best_model.pt\"))\n",
    "mm.to('cuda')\n",
    "softmax_nodes = [n for n in get_graph_node_names(mm)[0] if 'softmax' in n]\n",
    "ff = timm.models.create_feature_extractor(mm, softmax_nodes)\n",
    "ff.to('cuda')\n",
    "#with torch.no_grad():\n",
    "#    output = ff(torch.randn(768, 1, 224, 224))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ff(input_tensor)\n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/rwightman/dbb5a8222df173687d734ad5e257908b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blocks.0.attn.softmax': tensor([[[[1.0000e+00, 1.6815e-31],\n",
      "          [1.0000e+00, 7.2125e-24]],\n",
      "\n",
      "         [[1.0000e+00, 2.5425e-15],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 3.7143e-10],\n",
      "          [3.5318e-09, 1.0000e+00]],\n",
      "\n",
      "         [[9.9959e-01, 4.1461e-04],\n",
      "          [1.1311e-22, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 2.0082e-31],\n",
      "          [1.0000e+00, 9.9221e-23]],\n",
      "\n",
      "         [[1.0000e+00, 1.4859e-08],\n",
      "          [2.5809e-09, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 1.6077e-12],\n",
      "          [1.5816e-21, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 2.5624e-16],\n",
      "          [2.5563e-31, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 4.4911e-12],\n",
      "          [1.4378e-03, 9.9856e-01]],\n",
      "\n",
      "         [[1.0000e+00, 1.5622e-07],\n",
      "          [4.4129e-01, 5.5871e-01]],\n",
      "\n",
      "         [[1.0000e+00, 8.4027e-07],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 1.4518e-22],\n",
      "          [8.1098e-20, 1.0000e+00]]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), 'blocks.1.attn.softmax': tensor([[[[1.4310e-01, 8.5690e-01],\n",
      "          [9.9999e-01, 7.9281e-06]],\n",
      "\n",
      "         [[1.0000e+00, 7.2727e-43],\n",
      "          [1.0000e+00, 3.6164e-27]],\n",
      "\n",
      "         [[2.6921e-20, 1.0000e+00],\n",
      "          [8.8910e-19, 1.0000e+00]],\n",
      "\n",
      "         [[1.7265e-07, 1.0000e+00],\n",
      "          [7.0476e-16, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 4.2039e-45],\n",
      "          [1.0000e+00, 3.9644e-17]],\n",
      "\n",
      "         [[1.0000e+00, 2.9305e-28],\n",
      "          [1.2073e-10, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 4.3603e-31]],\n",
      "\n",
      "         [[1.6242e-04, 9.9984e-01],\n",
      "          [1.0000e+00, 6.0259e-09]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 8.3938e-43]],\n",
      "\n",
      "         [[3.0155e-29, 1.0000e+00],\n",
      "          [1.2743e-34, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 1.7606e-26],\n",
      "          [9.7851e-01, 2.1490e-02]]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), 'blocks.2.attn.softmax': tensor([[[[1.0000e+00, 4.3184e-26],\n",
      "          [1.0000e+00, 4.4185e-33]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 1.0004e-41]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[2.1477e-18, 1.0000e+00],\n",
      "          [3.9073e-02, 9.6093e-01]],\n",
      "\n",
      "         [[1.9592e-10, 1.0000e+00],\n",
      "          [2.7886e-11, 1.0000e+00]],\n",
      "\n",
      "         [[7.3191e-25, 1.0000e+00],\n",
      "          [1.3510e-09, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[9.1058e-07, 1.0000e+00],\n",
      "          [1.0000e+00, 9.3585e-07]],\n",
      "\n",
      "         [[6.2896e-37, 1.0000e+00],\n",
      "          [4.0613e-33, 1.0000e+00]]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), 'blocks.3.attn.softmax': tensor([[[[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [1.8427e-35, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [5.3849e-15, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 1.1939e-21]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 2.6629e-18],\n",
      "          [1.0000e+00, 6.5382e-25]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), 'blocks.4.attn.softmax': tensor([[[[2.8026e-45, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[9.9011e-01, 9.8918e-03],\n",
      "          [1.0000e+00, 1.2008e-12]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 2.1349e-14],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.2850e-26, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), 'blocks.5.attn.softmax': tensor([[[[1.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[8.7965e-30, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[3.2957e-07, 1.0000e+00],\n",
      "          [1.0000e+00, 3.9611e-34]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[2.5841e-30, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 1.4246e-30],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [8.7921e-13, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), 'blocks.6.attn.softmax': tensor([[[[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 7.9326e-38],\n",
      "          [1.9532e-24, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [9.9052e-01, 9.4794e-03]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 2.3822e-44],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 5.4651e-44],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[4.2702e-29, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), 'blocks.7.attn.softmax': tensor([[[[5.6052e-45, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [1.3023e-28, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), 'blocks.8.attn.softmax': tensor([[[[1.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 7.4123e-15],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 4.4595e-22]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 1.7236e-43]]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), 'blocks.9.attn.softmax': tensor([[[[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[5.0593e-33, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 6.1517e-20],\n",
      "          [1.0000e+00, 1.4126e-31]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>), 'blocks.10.attn.softmax': tensor([[[[1., 0.],\n",
      "          [0., 1.]],\n",
      "\n",
      "         [[0., 1.],\n",
      "          [1., 0.]],\n",
      "\n",
      "         [[1., 0.],\n",
      "          [0., 1.]],\n",
      "\n",
      "         [[1., 0.],\n",
      "          [0., 1.]],\n",
      "\n",
      "         [[0., 1.],\n",
      "          [1., 0.]],\n",
      "\n",
      "         [[0., 1.],\n",
      "          [0., 1.]],\n",
      "\n",
      "         [[0., 1.],\n",
      "          [1., 0.]],\n",
      "\n",
      "         [[0., 1.],\n",
      "          [0., 1.]],\n",
      "\n",
      "         [[1., 0.],\n",
      "          [0., 1.]],\n",
      "\n",
      "         [[0., 1.],\n",
      "          [1., 0.]],\n",
      "\n",
      "         [[0., 1.],\n",
      "          [0., 1.]],\n",
      "\n",
      "         [[0., 1.],\n",
      "          [0., 1.]]]], device='cuda:0', grad_fn=<SoftmaxBackward0>), 'blocks.11.attn.softmax': tensor([[[[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[7.1943e-42, 1.0000e+00],\n",
      "          [3.8812e-07, 1.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0212e-34, 1.0000e+00],\n",
      "          [1.0000e+00, 1.9478e-43]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e+00],\n",
      "          [0.0000e+00, 1.0000e+00]]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "\n",
    "output['blocks.11.attn.softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer_attention = output[:-1]  # Last attention layer\n",
    "# Average across heads and reshape\n",
    "attention_map = last_layer_attention.squeeze(0).mean(dim=0).detach().cpu().numpy()\n",
    "attention_map = attention_map.reshape(14, 14)  # Reshape to (H, W)\n",
    "\n",
    "# Visualize the attention map\n",
    "plt.imshow(attention_map, cmap=\"viridis\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Attention Map from Last Layer\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
