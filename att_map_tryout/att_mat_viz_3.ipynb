{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 1: Load the Model\n",
    "num_output_classes = 24  # Same as used during training\n",
    "\n",
    "mm = timm.create_model(\n",
    "     \"vit_base_patch8_224\",\n",
    "     pretrained=True,\n",
    "     in_chans=1,\n",
    "     num_classes=num_output_classes,\n",
    "     patch_size=128,) #change this to 224, 128, 64\n",
    "\n",
    "mm.load_state_dict(torch.load(\"models/best_model_128.pt\", map_location=torch.device('cpu'))) #change this to 64, 128, 224\n",
    "#mm.eval()\n",
    "#mm = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(128, 128), stride=(128, 128))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (patch_drop): Identity()\n",
      "  (norm_pre): Identity()\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (fc_norm): Identity()\n",
      "  (head_drop): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=768, out_features=24, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee = AttentionExtract(mm, method='fx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the Image\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"L\")  # Convert to grayscale\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to the model's input size\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(mean=[0.5], std=[0.5]),  # Normalize for grayscale\n",
    "    ])\n",
    "    input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return input_tensor\n",
    "\n",
    "#Forward Pass to Extract Attention Maps\n",
    "image_path = \"/Users/annastuckert/Documents/GitHub/facemap_avs/first_image.jpg\"  # Replace with your image path\n",
    "input_tensor = preprocess_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ee(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_mat =output['blocks.11.attn.softmax']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(att_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Your attention tensor\n",
    "attention_tensor = torch.tensor([[[[1.0000e+00, 7.8763e-24],\n",
    "                                     [1.0000e+00, 3.3209e-25]],\n",
    "\n",
    "                                    [[8.7875e-20, 1.0000e+00],\n",
    "                                     [5.9862e-24, 1.0000e+00]],\n",
    "\n",
    "                                    [[1.0000e+00, 1.6058e-37],\n",
    "                                     [1.0000e+00, 2.0562e-40]],\n",
    "\n",
    "                                    [[9.9975e-01, 2.5016e-04],\n",
    "                                     [1.0000e+00, 1.6065e-06]],\n",
    "\n",
    "                                    [[1.0000e+00, 2.3684e-15],\n",
    "                                     [1.0000e+00, 1.0344e-17]],\n",
    "\n",
    "                                    [[2.1024e-01, 7.8976e-01],\n",
    "                                     [3.1610e-01, 6.8390e-01]],\n",
    "\n",
    "                                    [[4.1398e-14, 1.0000e+00],\n",
    "                                     [2.5753e-17, 1.0000e+00]],\n",
    "\n",
    "                                    [[1.3607e-03, 9.9864e-01],\n",
    "                                     [3.9586e-04, 9.9960e-01]],\n",
    "\n",
    "                                    [[6.3755e-12, 1.0000e+00],\n",
    "                                     [3.1041e-14, 1.0000e+00]],\n",
    "\n",
    "                                    [[9.9493e-01, 5.0731e-03],\n",
    "                                     [9.9989e-01, 1.0777e-04]],\n",
    "\n",
    "                                    [[1.7313e-15, 1.0000e+00],\n",
    "                                     [4.5229e-18, 1.0000e+00]],\n",
    "\n",
    "                                    [[1.0000e+00, 6.6104e-36],\n",
    "                                     [1.0000e+00, 1.4262e-38]]]])\n",
    "\n",
    "# Average the attention values and normalize for visualization\n",
    "attention_map = attention_tensor.mean(dim=1).numpy()\n",
    "attention_map_normalized = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min())\n",
    "\n",
    "# Load and resize the image\n",
    "image_path = '/Users/annastuckert/Documents/GitHub/facemap_avs/first_image.jpg'  # Update to your image path\n",
    "image = Image.open(image_path).resize((attention_map.shape[1], attention_map.shape[2]))\n",
    "\n",
    "# Convert image to numpy array\n",
    "image_array = np.array(image) / 255.0  # Normalize pixel values\n",
    "\n",
    "# Create a red heatmap\n",
    "heatmap = np.zeros((*attention_map.shape[1:], 3))\n",
    "heatmap[..., 0] = attention_map_normalized  # Red channel\n",
    "\n",
    "# Overlay heatmap on the image\n",
    "overlay = 0.5 * image_array + 0.5 * heatmap\n",
    "\n",
    "# Display the results\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(overlay)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.attn.softmax torch.Size([1, 12, 2, 2])\n",
      "blocks.1.attn.softmax torch.Size([1, 12, 2, 2])\n",
      "blocks.2.attn.softmax torch.Size([1, 12, 2, 2])\n",
      "blocks.3.attn.softmax torch.Size([1, 12, 2, 2])\n",
      "blocks.4.attn.softmax torch.Size([1, 12, 2, 2])\n",
      "blocks.5.attn.softmax torch.Size([1, 12, 2, 2])\n",
      "blocks.6.attn.softmax torch.Size([1, 12, 2, 2])\n",
      "blocks.7.attn.softmax torch.Size([1, 12, 2, 2])\n",
      "blocks.8.attn.softmax torch.Size([1, 12, 2, 2])\n",
      "blocks.9.attn.softmax torch.Size([1, 12, 2, 2])\n",
      "blocks.10.attn.softmax torch.Size([1, 12, 2, 2])\n",
      "blocks.11.attn.softmax torch.Size([1, 12, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "for n, t in output.items():\n",
    "    print(n, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POssibly interesting resources https://github.com/facebookresearch/dino/blob/main/visualize_attention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VisionTransformer:\n\tsize mismatch for pos_embed: copying a param with shape torch.Size([1, 2, 768]) from checkpoint, the shape in current model is torch.Size([1, 197, 768]).\n\tsize mismatch for patch_embed.proj.weight: copying a param with shape torch.Size([768, 1, 224, 224]) from checkpoint, the shape in current model is torch.Size([768, 3, 16, 16]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([24, 768]) from checkpoint, the shape in current model is torch.Size([1000, 768]).\n\tsize mismatch for head.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([1000]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m timm\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mset_fused_attn(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m mm \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mcreate_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvit_base_patch16_224\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m mm\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/best_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m)\n\u001b[1;32m      8\u001b[0m ee \u001b[38;5;241m=\u001b[39m AttentionExtract(mm, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VisionTransformer:\n\tsize mismatch for pos_embed: copying a param with shape torch.Size([1, 2, 768]) from checkpoint, the shape in current model is torch.Size([1, 197, 768]).\n\tsize mismatch for patch_embed.proj.weight: copying a param with shape torch.Size([768, 1, 224, 224]) from checkpoint, the shape in current model is torch.Size([768, 3, 16, 16]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([24, 768]) from checkpoint, the shape in current model is torch.Size([1000, 768]).\n\tsize mismatch for head.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([1000])."
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "from timm.utils import AttentionExtract\n",
    "timm.layers.set_fused_attn(False)\n",
    "mm = timm.create_model('vit_base_patch16_224')\n",
    "#mm.load_state_dict(torch.load(\"models/best_model.pt\", map_location=torch.device('cpu')))\n",
    "input = torch.randn(2,3,224,224)\n",
    "ee = AttentionExtract(mm, method='fx')\n",
    "oo = ee(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0057, 0.0050, 0.0054,  ..., 0.0069, 0.0036, 0.0039],\n",
       "          [0.0047, 0.0053, 0.0031,  ..., 0.0077, 0.0032, 0.0052],\n",
       "          [0.0061, 0.0053, 0.0038,  ..., 0.0059, 0.0059, 0.0046],\n",
       "          ...,\n",
       "          [0.0045, 0.0070, 0.0041,  ..., 0.0065, 0.0043, 0.0057],\n",
       "          [0.0077, 0.0048, 0.0049,  ..., 0.0044, 0.0063, 0.0042],\n",
       "          [0.0051, 0.0060, 0.0036,  ..., 0.0074, 0.0064, 0.0042]],\n",
       "\n",
       "         [[0.0039, 0.0053, 0.0121,  ..., 0.0056, 0.0053, 0.0059],\n",
       "          [0.0057, 0.0048, 0.0063,  ..., 0.0038, 0.0045, 0.0065],\n",
       "          [0.0056, 0.0040, 0.0050,  ..., 0.0049, 0.0033, 0.0066],\n",
       "          ...,\n",
       "          [0.0038, 0.0031, 0.0063,  ..., 0.0048, 0.0059, 0.0037],\n",
       "          [0.0048, 0.0044, 0.0053,  ..., 0.0048, 0.0041, 0.0038],\n",
       "          [0.0049, 0.0021, 0.0046,  ..., 0.0042, 0.0061, 0.0049]],\n",
       "\n",
       "         [[0.0044, 0.0067, 0.0078,  ..., 0.0071, 0.0052, 0.0045],\n",
       "          [0.0062, 0.0037, 0.0031,  ..., 0.0054, 0.0052, 0.0065],\n",
       "          [0.0046, 0.0076, 0.0048,  ..., 0.0044, 0.0064, 0.0028],\n",
       "          ...,\n",
       "          [0.0062, 0.0047, 0.0061,  ..., 0.0063, 0.0062, 0.0042],\n",
       "          [0.0061, 0.0056, 0.0058,  ..., 0.0065, 0.0046, 0.0058],\n",
       "          [0.0041, 0.0071, 0.0059,  ..., 0.0050, 0.0048, 0.0057]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0063, 0.0038, 0.0057,  ..., 0.0105, 0.0032, 0.0057],\n",
       "          [0.0036, 0.0043, 0.0060,  ..., 0.0097, 0.0038, 0.0054],\n",
       "          [0.0048, 0.0031, 0.0038,  ..., 0.0057, 0.0056, 0.0053],\n",
       "          ...,\n",
       "          [0.0038, 0.0052, 0.0037,  ..., 0.0040, 0.0067, 0.0063],\n",
       "          [0.0059, 0.0034, 0.0064,  ..., 0.0058, 0.0056, 0.0043],\n",
       "          [0.0045, 0.0050, 0.0053,  ..., 0.0067, 0.0042, 0.0058]],\n",
       "\n",
       "         [[0.0045, 0.0045, 0.0045,  ..., 0.0067, 0.0043, 0.0035],\n",
       "          [0.0052, 0.0048, 0.0062,  ..., 0.0082, 0.0054, 0.0048],\n",
       "          [0.0039, 0.0050, 0.0057,  ..., 0.0051, 0.0069, 0.0051],\n",
       "          ...,\n",
       "          [0.0058, 0.0042, 0.0057,  ..., 0.0091, 0.0044, 0.0040],\n",
       "          [0.0036, 0.0035, 0.0074,  ..., 0.0047, 0.0069, 0.0047],\n",
       "          [0.0035, 0.0043, 0.0032,  ..., 0.0041, 0.0040, 0.0047]],\n",
       "\n",
       "         [[0.0050, 0.0056, 0.0044,  ..., 0.0030, 0.0067, 0.0045],\n",
       "          [0.0066, 0.0046, 0.0067,  ..., 0.0051, 0.0057, 0.0042],\n",
       "          [0.0068, 0.0068, 0.0054,  ..., 0.0047, 0.0050, 0.0060],\n",
       "          ...,\n",
       "          [0.0061, 0.0048, 0.0052,  ..., 0.0033, 0.0052, 0.0049],\n",
       "          [0.0049, 0.0100, 0.0034,  ..., 0.0041, 0.0035, 0.0027],\n",
       "          [0.0048, 0.0053, 0.0048,  ..., 0.0047, 0.0037, 0.0056]]],\n",
       "\n",
       "\n",
       "        [[[0.0056, 0.0044, 0.0054,  ..., 0.0055, 0.0056, 0.0047],\n",
       "          [0.0058, 0.0059, 0.0046,  ..., 0.0069, 0.0056, 0.0040],\n",
       "          [0.0049, 0.0032, 0.0062,  ..., 0.0044, 0.0049, 0.0026],\n",
       "          ...,\n",
       "          [0.0051, 0.0033, 0.0047,  ..., 0.0054, 0.0054, 0.0041],\n",
       "          [0.0044, 0.0030, 0.0062,  ..., 0.0037, 0.0063, 0.0047],\n",
       "          [0.0055, 0.0051, 0.0038,  ..., 0.0043, 0.0061, 0.0051]],\n",
       "\n",
       "         [[0.0051, 0.0037, 0.0040,  ..., 0.0055, 0.0044, 0.0042],\n",
       "          [0.0049, 0.0035, 0.0051,  ..., 0.0047, 0.0042, 0.0038],\n",
       "          [0.0058, 0.0037, 0.0043,  ..., 0.0076, 0.0069, 0.0038],\n",
       "          ...,\n",
       "          [0.0057, 0.0059, 0.0045,  ..., 0.0055, 0.0041, 0.0048],\n",
       "          [0.0050, 0.0040, 0.0052,  ..., 0.0049, 0.0051, 0.0043],\n",
       "          [0.0057, 0.0046, 0.0066,  ..., 0.0054, 0.0060, 0.0045]],\n",
       "\n",
       "         [[0.0042, 0.0046, 0.0060,  ..., 0.0064, 0.0035, 0.0048],\n",
       "          [0.0045, 0.0046, 0.0045,  ..., 0.0060, 0.0038, 0.0051],\n",
       "          [0.0030, 0.0041, 0.0069,  ..., 0.0049, 0.0079, 0.0038],\n",
       "          ...,\n",
       "          [0.0054, 0.0030, 0.0072,  ..., 0.0055, 0.0045, 0.0055],\n",
       "          [0.0031, 0.0053, 0.0060,  ..., 0.0038, 0.0048, 0.0051],\n",
       "          [0.0029, 0.0038, 0.0059,  ..., 0.0060, 0.0037, 0.0067]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0040, 0.0033, 0.0050,  ..., 0.0039, 0.0052, 0.0049],\n",
       "          [0.0036, 0.0030, 0.0073,  ..., 0.0037, 0.0059, 0.0048],\n",
       "          [0.0044, 0.0079, 0.0047,  ..., 0.0056, 0.0055, 0.0056],\n",
       "          ...,\n",
       "          [0.0062, 0.0036, 0.0055,  ..., 0.0041, 0.0066, 0.0054],\n",
       "          [0.0033, 0.0049, 0.0043,  ..., 0.0043, 0.0052, 0.0068],\n",
       "          [0.0048, 0.0043, 0.0044,  ..., 0.0057, 0.0071, 0.0048]],\n",
       "\n",
       "         [[0.0064, 0.0028, 0.0042,  ..., 0.0044, 0.0051, 0.0064],\n",
       "          [0.0041, 0.0043, 0.0067,  ..., 0.0046, 0.0054, 0.0058],\n",
       "          [0.0053, 0.0057, 0.0083,  ..., 0.0045, 0.0051, 0.0037],\n",
       "          ...,\n",
       "          [0.0040, 0.0053, 0.0046,  ..., 0.0049, 0.0060, 0.0091],\n",
       "          [0.0038, 0.0041, 0.0073,  ..., 0.0056, 0.0052, 0.0098],\n",
       "          [0.0047, 0.0047, 0.0049,  ..., 0.0040, 0.0034, 0.0046]],\n",
       "\n",
       "         [[0.0046, 0.0030, 0.0051,  ..., 0.0038, 0.0039, 0.0054],\n",
       "          [0.0056, 0.0073, 0.0048,  ..., 0.0051, 0.0038, 0.0083],\n",
       "          [0.0057, 0.0063, 0.0051,  ..., 0.0038, 0.0039, 0.0030],\n",
       "          ...,\n",
       "          [0.0049, 0.0041, 0.0039,  ..., 0.0053, 0.0039, 0.0033],\n",
       "          [0.0052, 0.0046, 0.0044,  ..., 0.0047, 0.0051, 0.0072],\n",
       "          [0.0063, 0.0042, 0.0055,  ..., 0.0054, 0.0041, 0.0044]]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oo['blocks.11.attn.softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.attn.softmax torch.Size([2, 12, 197, 197])\n",
      "blocks.1.attn.softmax torch.Size([2, 12, 197, 197])\n",
      "blocks.2.attn.softmax torch.Size([2, 12, 197, 197])\n",
      "blocks.3.attn.softmax torch.Size([2, 12, 197, 197])\n",
      "blocks.4.attn.softmax torch.Size([2, 12, 197, 197])\n",
      "blocks.5.attn.softmax torch.Size([2, 12, 197, 197])\n",
      "blocks.6.attn.softmax torch.Size([2, 12, 197, 197])\n",
      "blocks.7.attn.softmax torch.Size([2, 12, 197, 197])\n",
      "blocks.8.attn.softmax torch.Size([2, 12, 197, 197])\n",
      "blocks.9.attn.softmax torch.Size([2, 12, 197, 197])\n",
      "blocks.10.attn.softmax torch.Size([2, 12, 197, 197])\n",
      "blocks.11.attn.softmax torch.Size([2, 12, 197, 197])\n"
     ]
    }
   ],
   "source": [
    "for n, t in oo.items():\n",
    "    print(n, t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)  # Convert to grayscale\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to the model's input size\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(mean=[0.5], std=[0.5]),  # Normalize for grayscale\n",
    "    ])\n",
    "    input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/Users/annastuckert/Documents/GitHub/facemap_avs/cam1_G7c1_1_img1383_pad_rescale_augmented.jpg\"  # Replace with your image path\n",
    "input_tensor = preprocess_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= ee(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.attn.softmax torch.Size([1, 12, 197, 197])\n",
      "blocks.1.attn.softmax torch.Size([1, 12, 197, 197])\n",
      "blocks.2.attn.softmax torch.Size([1, 12, 197, 197])\n",
      "blocks.3.attn.softmax torch.Size([1, 12, 197, 197])\n",
      "blocks.4.attn.softmax torch.Size([1, 12, 197, 197])\n",
      "blocks.5.attn.softmax torch.Size([1, 12, 197, 197])\n",
      "blocks.6.attn.softmax torch.Size([1, 12, 197, 197])\n",
      "blocks.7.attn.softmax torch.Size([1, 12, 197, 197])\n",
      "blocks.8.attn.softmax torch.Size([1, 12, 197, 197])\n",
      "blocks.9.attn.softmax torch.Size([1, 12, 197, 197])\n",
      "blocks.10.attn.softmax torch.Size([1, 12, 197, 197])\n",
      "blocks.11.attn.softmax torch.Size([1, 12, 197, 197])\n"
     ]
    }
   ],
   "source": [
    "for n, t in output.items():\n",
    "    print(n, t.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
